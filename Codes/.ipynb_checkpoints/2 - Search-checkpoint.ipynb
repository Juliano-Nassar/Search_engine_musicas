{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.indexer import load_index\n",
    "from Modules.cleaner import Cleaner\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATH = '../Data/index/index.json'\n",
    "index = load_index(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSICS_PARSED_PATH = '../Data/music_info/musics_parsed.csv'\n",
    "musics_parsed = pd.read_csv(MUSICS_PARSED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = Cleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=False,reduce_len=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_string(string):\n",
    "    pattern = r'(\\(|\\))'\n",
    "    return re.sub(pattern,r' \\1 ',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_and_index_query(x,index):\n",
    "    if type(x) == str:\n",
    "        if x in index.keys():\n",
    "            index_dict = index[x]\n",
    "            index_query = set(index_dict)\n",
    "            full_dict_querry = index_dict\n",
    "        else:\n",
    "            full_dict_querry = {}\n",
    "            index_query = set([])\n",
    "        return full_dict_querry, index_query\n",
    "    else:\n",
    "        index_query = set(x)\n",
    "        return x, index_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song(music_index,music_parsed):\n",
    "    return cleaner.clean_and_tokenize(music_parsed.loc[music_index]['Lyric'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND_query(index,query):\n",
    "    \n",
    "    dict_querry,index_query = get_dict_and_index_query(query[0],index)\n",
    "    full_dict_querry = [dict_querry]\n",
    "    \n",
    "    \n",
    "    for query_term in query[1:]:\n",
    "        dict_querry,term_index_query = get_dict_and_index_query(query_term,index)\n",
    "        index_query &= term_index_query\n",
    "        full_dict_querry.append(dict_querry)\n",
    "        \n",
    "    dict_out = defaultdict(list)\n",
    "    for doc in index_query:\n",
    "        for current_dict in full_dict_querry:\n",
    "            if doc in current_dict.keys():\n",
    "                dict_out[doc]+=current_dict[doc]\n",
    "        \n",
    "    return dict_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_query(index,query):\n",
    "    \n",
    "    dict_querry,index_query = get_dict_and_index_query(query[0],index)\n",
    "    full_dict_querry = [dict_querry]\n",
    "    \n",
    "    \n",
    "    for query_term in query[1:]:\n",
    "        dict_querry,term_index_query = get_dict_and_index_query(query_term,index)\n",
    "        index_query |= term_index_query\n",
    "        full_dict_querry.append(dict_querry)\n",
    "        \n",
    "    dict_out = defaultdict(list)\n",
    "    for doc in index_query:\n",
    "        for current_dict in full_dict_querry:\n",
    "            if doc in current_dict.keys():\n",
    "                dict_out[doc]+=current_dict[doc]\n",
    "        \n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_query(index,term):\n",
    "    \n",
    "    dict_querry,index_query = get_dict_and_index_query(term,index)\n",
    "    \n",
    "    okk = set()\n",
    "    for i in index.values():\n",
    "        okk |= set(i)\n",
    "    \n",
    "    index_query = okk-index_query\n",
    "        \n",
    "    dict_out = defaultdict(list)\n",
    "    for doc in index_query:\n",
    "        dict_out[doc] = []\n",
    "        \n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEAR_query(index,query):\n",
    "    \n",
    "    dict_querry_1,index_query_1 = get_dict_and_index_query(query[0],index)\n",
    "    dict_querry_2,index_query_2 = get_dict_and_index_query(query[1],index)\n",
    "    \n",
    "    index_query_1 &= index_query_2\n",
    "    \n",
    "    dict_out = defaultdict(list)\n",
    "    \n",
    "    for doc in index_query_1:\n",
    "        vals_2 = dict_querry_2[doc]\n",
    "        vals_1 = dict_querry_1[doc]\n",
    "        for val_1 in vals_1:\n",
    "            if (val_1+1 in vals_2) or (val_1-1 in vals_2):\n",
    "                dict_out[doc]= vals_1 + vals_2\n",
    "                break\n",
    "        \n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_query(index,query):\n",
    "    query_tokens = query.copy()\n",
    "    while 'NOT' in query_tokens:\n",
    "        NOT_index = query_tokens.index('NOT')\n",
    "        token = query_tokens[NOT_index+1]\n",
    "        if type(token) == str:\n",
    "            token = cleaner.clean_and_tokenize(token)\n",
    "            if len(token) >1:\n",
    "                token = AND_query(index,token)\n",
    "            elif len(token) ==0:\n",
    "                del query_tokens[NOT_index:NOT_index+2]\n",
    "                pass\n",
    "        \n",
    "        parsed_token = NOT_query(index,token)\n",
    "        \n",
    "        del query_tokens[NOT_index:NOT_index+2]\n",
    "        \n",
    "        query_tokens.insert(NOT_index,parsed_token)\n",
    " \n",
    "    while 'NEAR' in query_tokens:\n",
    "        NEAR_index = query_tokens.index('NEAR')\n",
    "        \n",
    "        token_1 = query_tokens[NEAR_index+1]\n",
    "        token_2 = query_tokens[NEAR_index-1]\n",
    "        \n",
    "        if type(token_1) == str:\n",
    "                token_1 = cleaner.clean_and_tokenize(token_1)\n",
    "                if len(token_1)>1:\n",
    "                    token_1 = AND_query(index,token_1)\n",
    "                else:\n",
    "                    token_1 = token_1[0]\n",
    "        \n",
    "        if type(token_2) == str:\n",
    "                token_2 = cleaner.clean_and_tokenize(token_2)\n",
    "                if len(token_2)>1:\n",
    "                    token_2 = AND_query(index,token_2)\n",
    "                else:\n",
    "                    token_2 = token_2[0]\n",
    "        \n",
    "        token = [token_1,token_2]\n",
    "        \n",
    "        parsed_token = NEAR_query(index,token)\n",
    "        \n",
    "        del query_tokens[NEAR_index-1:NEAR_index+2]\n",
    "        \n",
    "        query_tokens.insert(NEAR_index-1,parsed_token)\n",
    "\n",
    "    while 'OR' in query_tokens:\n",
    "        OR_index = query_tokens.index('OR')\n",
    "        \n",
    "        token_1 = query_tokens[OR_index+1]\n",
    "        token_2 = query_tokens[OR_index-1]\n",
    "        \n",
    "        if type(token_1) == str:\n",
    "                token_1 = cleaner.clean_and_tokenize(token_1)\n",
    "                if len(token_1)>1:\n",
    "                    token_1 = AND_query(index,token_1)\n",
    "                else:\n",
    "                    token_1 = token_1[0]\n",
    "        \n",
    "        if type(token_2) == str:\n",
    "                token_2 = cleaner.clean_and_tokenize(token_2)\n",
    "                if len(token_2)>1:\n",
    "                    token_2 = AND_query(index,token_2)\n",
    "                else:\n",
    "                    token_2 = token_2[0]\n",
    "        \n",
    "        token = [token_1,token_2]\n",
    "        \n",
    "        parsed_token = OR_query(index,token)\n",
    "        \n",
    "        del query_tokens[OR_index-1:OR_index+2]\n",
    "        \n",
    "        query_tokens.insert(OR_index-1,parsed_token)\n",
    "    \n",
    "    if len(query_tokens)>1:\n",
    "        final_query = []\n",
    "        \n",
    "        for token in query_tokens:\n",
    "            if type(token) == str:\n",
    "                final_query += cleaner.clean_and_tokenize(token)\n",
    "            else:\n",
    "                final_query.append(token)\n",
    "        \n",
    "        query_tokens = AND_query(index,final_query)\n",
    "        \n",
    "    if type(query_tokens) == list:\n",
    "        query_tokens = query_tokens[0]\n",
    "        \n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_query(query):\n",
    "    query = easy_string(query)\n",
    "    query = tokenizer.tokenize(query)\n",
    "    \n",
    "    lista = query.copy()\n",
    "    aux = 0\n",
    "    pos_i = 0\n",
    "    \n",
    "    while True:\n",
    "        char = lista[aux]\n",
    "        if char == '(':\n",
    "            pos_i = aux\n",
    "        if char == ')':\n",
    "            result = solve_query(index,lista[pos_i+1:aux])\n",
    "            del lista[pos_i:aux+1]\n",
    "            \n",
    "            lista.insert(pos_i,result)\n",
    "            \n",
    "            aux = 0\n",
    "        aux+=1\n",
    "        if aux== len(lista):\n",
    "            break\n",
    "    \n",
    "    if len(lista)>1:\n",
    "        lista = solve_query(index,lista)\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = \"NOT 30$40 OR NOT 70 oi NEAR linda\"\n",
    "teste_2 = \"(Moça NEAR bonita) OR R7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = parse_query(teste_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_song(139608,musics_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(doc_number,word_info,N):\n",
    "    if doc_number in word_info.keys():\n",
    "        fij = len(word_info[doc_number])\n",
    "    else:\n",
    "        fij = 0\n",
    "        \n",
    "    ni = len(word_info.keys())\n",
    "    \n",
    "    return np.log2(1 + fij)*np.log2(N/ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf('2465',index['oi'],len(musics_parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
